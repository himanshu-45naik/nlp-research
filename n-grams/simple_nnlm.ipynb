{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfcae2d",
   "metadata": {},
   "source": [
    "### Implementing neural network model using Bigram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f93bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a77ff9",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "- this data consists of names -> this data is taken from Andrej Karpathy's makeamore playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa41ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c30e8",
   "metadata": {},
   "source": [
    "### Converting the txt file into list of names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e653bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e32e9",
   "metadata": {},
   "source": [
    "### Mapping each unique character to an index\n",
    "#### '.' indicates start/end of a name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9948589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch:ind+1 for ind,ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308c70b",
   "metadata": {},
   "source": [
    "### Mapping integers(index above) to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4572b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e11d23",
   "metadata": {},
   "source": [
    "# Creating Dataset for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e598b1",
   "metadata": {},
   "source": [
    "- Here we will check for first name emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6672f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# x -> characters preceding the target character\n",
    "xs = []\n",
    "\n",
    "# y label -> target character\n",
    "ys = []\n",
    "\n",
    "# Check for only first name emma\n",
    "for name in names[:1]:\n",
    "    chs = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fcded",
   "metadata": {},
   "source": [
    "#### X tensor consists of Preceding characters in the name\n",
    "#### Y tensor consists of correspoding target/next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bf800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preceding characters\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5aa449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels for xs() - Target\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccc5ab",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "- Using OHE -> an array of size 27(equal to vocab) , such that the index with value 1 indicates the input charater\n",
    "- Ex => e -> [0, 0, 0, 0, 0, 1, 0, 0,..............0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5e7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ee42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding of each word\n",
    "# x_enc must be encoded as float32 \n",
    "# For operations in neural network requires float datatype\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7869f5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25565daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33c330",
   "metadata": {},
   "source": [
    "# Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf781d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0456, 0.0111, 0.0075, 0.1967, 0.0139, 0.0028, 0.0722, 0.0170, 0.1018,\n",
       "         0.0150, 0.0578, 0.0101, 0.0223, 0.0098, 0.0222, 0.0389, 0.0054, 0.0123,\n",
       "         0.0219, 0.0043, 0.0378, 0.0072, 0.1458, 0.0089, 0.0116, 0.0969, 0.0032],\n",
       "        [0.0552, 0.0287, 0.0577, 0.0028, 0.0097, 0.0162, 0.0087, 0.1396, 0.0682,\n",
       "         0.0889, 0.1469, 0.0168, 0.0457, 0.0352, 0.0125, 0.0380, 0.0120, 0.0221,\n",
       "         0.0065, 0.0280, 0.0113, 0.0344, 0.0118, 0.0158, 0.0034, 0.0525, 0.0313],\n",
       "        [0.1147, 0.0707, 0.0142, 0.0128, 0.0733, 0.0134, 0.0374, 0.0028, 0.0156,\n",
       "         0.0709, 0.0171, 0.0044, 0.1151, 0.0352, 0.0074, 0.0043, 0.2330, 0.0166,\n",
       "         0.0153, 0.0096, 0.0140, 0.0052, 0.0132, 0.0444, 0.0255, 0.0108, 0.0031],\n",
       "        [0.1147, 0.0707, 0.0142, 0.0128, 0.0733, 0.0134, 0.0374, 0.0028, 0.0156,\n",
       "         0.0709, 0.0171, 0.0044, 0.1151, 0.0352, 0.0074, 0.0043, 0.2330, 0.0166,\n",
       "         0.0153, 0.0096, 0.0140, 0.0052, 0.0132, 0.0444, 0.0255, 0.0108, 0.0031],\n",
       "        [0.0845, 0.0187, 0.0323, 0.0589, 0.0267, 0.0306, 0.0550, 0.0284, 0.0208,\n",
       "         0.0013, 0.0075, 0.0652, 0.0145, 0.0304, 0.0427, 0.0233, 0.0803, 0.0233,\n",
       "         0.0119, 0.0898, 0.0223, 0.0197, 0.0349, 0.0241, 0.0069, 0.0068, 0.1392]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1st neuron\n",
    "# W = torch.randn((27,1)) # For one neuron\n",
    "\n",
    "# 1st neural layer with 27 neurons\n",
    "W = torch.randn((27,27))\n",
    "\n",
    "# Output of First layer\n",
    "x_enc @ W\n",
    "\n",
    "# (5,27) @ (27,1) => (5,1)\n",
    "# (5,27) @ (27,27) => (5,27)\n",
    "# (x_enc @ W).shape -> (5, 27)\n",
    "\n",
    "# This can be interpreted as same as counts in n-gram model\n",
    "#  NN is predicting nothing but counts\n",
    "logits = (x_enc @ W) # Log-counts\n",
    "counts = logits.exp()  # Equavalent to N matrix (n-gram counts in ngram model)\n",
    "\n",
    "probs = counts / counts.sum(1, keepdims=True)  # probs = torch.softmax(x_enc @ W, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02af02",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Here W is the C table consisting of word vectors for each word in the vocabulary\n",
    "- While x_enx @ W is the lookup\n",
    "\n",
    "#### x_enc @ W\n",
    "\n",
    "    - e -> [............d dimention vector................]  \n",
    "    - m -> [............d dimention vector................]  \n",
    "    - m -> [............d dimention vector................]  \n",
    "    - a -> [............d dimention vector................]  \n",
    "    - . -> [............d dimention vector................]  \n",
    "\n",
    "- here d = dimension of word vector\n",
    "-------------\n",
    "### This is nothing but Softmax\n",
    "```\n",
    "logits = (x_enc @ W).exp() # Log-counts\n",
    "counts = logits.exp()  # Equavalent to N matrix (n-gram counts in ngram model)\n",
    "counts\n",
    "```\n",
    "\n",
    "- We will take exponential of x_enc @ W -> Coz we want probabilities\n",
    "- `(x_enc @ W)` gives output ranging Real numbers (can be negative, positive, anything)\n",
    "- But we want probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb75b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6729)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_enc @ W)[3,13]  # O/P - tensor(-1.9677)\n",
    "\n",
    "(x_enc[3] * W[13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f970d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0456, 0.0111, 0.0075, 0.1967, 0.0139, 0.0028, 0.0722, 0.0170, 0.1018,\n",
      "        0.0150, 0.0578, 0.0101, 0.0223, 0.0098, 0.0222, 0.0389, 0.0054, 0.0123,\n",
      "        0.0219, 0.0043, 0.0378, 0.0072, 0.1458, 0.0089, 0.0116, 0.0969, 0.0032])\n",
      "torch.Size([27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows that for each there are certain number of possible output character\n",
    "# Al toghether it is equal to 1\n",
    "\n",
    "# We gave input '.' to our Neural Net\n",
    "# and we got the output -> which decides the possible next character\n",
    "print(probs[0])\n",
    "print(probs[0].shape)\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ac20b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1967)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe09ee",
   "metadata": {},
   "source": [
    "# Loss fucntion\n",
    "- We need to check how good our 1 layer Neural Network model work on randomly intialized weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f61df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram example 1: .e (indexes 05)\n",
      "input to neural net:  0\n",
      "output probabilities from the neural net:  tensor([0.0456, 0.0111, 0.0075, 0.1967, 0.0139, 0.0028, 0.0722, 0.0170, 0.1018,\n",
      "        0.0150, 0.0578, 0.0101, 0.0223, 0.0098, 0.0222, 0.0389, 0.0054, 0.0123,\n",
      "        0.0219, 0.0043, 0.0378, 0.0072, 0.1458, 0.0089, 0.0116, 0.0969, 0.0032])\n",
      "label(actual character):  5\n",
      "probability assigned by nn to the correct next charcater:  0.0027786234859377146\n",
      "Log likelihood:  -5.885799407958984\n",
      "Negative log likelihood:  5.885799407958984\n",
      "bigram example 2: em (indexes 513)\n",
      "input to neural net:  5\n",
      "output probabilities from the neural net:  tensor([0.0552, 0.0287, 0.0577, 0.0028, 0.0097, 0.0162, 0.0087, 0.1396, 0.0682,\n",
      "        0.0889, 0.1469, 0.0168, 0.0457, 0.0352, 0.0125, 0.0380, 0.0120, 0.0221,\n",
      "        0.0065, 0.0280, 0.0113, 0.0344, 0.0118, 0.0158, 0.0034, 0.0525, 0.0313])\n",
      "label(actual character):  13\n",
      "probability assigned by nn to the correct next charcater:  0.035170070827007294\n",
      "Log likelihood:  -3.347559928894043\n",
      "Negative log likelihood:  3.347559928894043\n",
      "bigram example 3: mm (indexes 1313)\n",
      "input to neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.1147, 0.0707, 0.0142, 0.0128, 0.0733, 0.0134, 0.0374, 0.0028, 0.0156,\n",
      "        0.0709, 0.0171, 0.0044, 0.1151, 0.0352, 0.0074, 0.0043, 0.2330, 0.0166,\n",
      "        0.0153, 0.0096, 0.0140, 0.0052, 0.0132, 0.0444, 0.0255, 0.0108, 0.0031])\n",
      "label(actual character):  13\n",
      "probability assigned by nn to the correct next charcater:  0.03517408296465874\n",
      "Log likelihood:  -3.3474457263946533\n",
      "Negative log likelihood:  3.3474457263946533\n",
      "bigram example 4: ma (indexes 131)\n",
      "input to neural net:  13\n",
      "output probabilities from the neural net:  tensor([0.1147, 0.0707, 0.0142, 0.0128, 0.0733, 0.0134, 0.0374, 0.0028, 0.0156,\n",
      "        0.0709, 0.0171, 0.0044, 0.1151, 0.0352, 0.0074, 0.0043, 0.2330, 0.0166,\n",
      "        0.0153, 0.0096, 0.0140, 0.0052, 0.0132, 0.0444, 0.0255, 0.0108, 0.0031])\n",
      "label(actual character):  1\n",
      "probability assigned by nn to the correct next charcater:  0.07074569910764694\n",
      "Log likelihood:  -2.6486635208129883\n",
      "Negative log likelihood:  2.6486635208129883\n",
      "bigram example 5: a. (indexes 10)\n",
      "input to neural net:  1\n",
      "output probabilities from the neural net:  tensor([0.0845, 0.0187, 0.0323, 0.0589, 0.0267, 0.0306, 0.0550, 0.0284, 0.0208,\n",
      "        0.0013, 0.0075, 0.0652, 0.0145, 0.0304, 0.0427, 0.0233, 0.0803, 0.0233,\n",
      "        0.0119, 0.0898, 0.0223, 0.0197, 0.0349, 0.0241, 0.0069, 0.0068, 0.1392])\n",
      "label(actual character):  0\n",
      "probability assigned by nn to the correct next charcater:  0.08447819203138351\n",
      "Log likelihood:  -2.471261978149414\n",
      "Negative log likelihood:  2.471261978149414\n",
      "=============\n",
      "avg negative log likelihood, i.e loss=  3.5401458740234375\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item()    # input character index\n",
    "    y = ys[i].item()    # label character index\n",
    "\n",
    "    print(f\"bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}{y})\")\n",
    "    print(\"input to neural net: \", x)\n",
    "    print(\"output probabilities from the neural net: \", probs[i])\n",
    "    print(\"label(actual character): \", y)\n",
    "\n",
    "    p = probs[i, y]\n",
    "    print(\"probability assigned by nn to the correct next charcater: \", p.item())\n",
    "    logp = torch.log(p)\n",
    "\n",
    "    print(\"Log likelihood: \", logp.item())\n",
    "    nll = -logp\n",
    "    print(\"Negative log likelihood: \", nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"avg negative log likelihood, i.e loss= \", nlls.mean().item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7be1c4",
   "metadata": {},
   "source": [
    "### Understanding likelihood\n",
    "\n",
    "- The probabilites in probs are nothing but P(yi|xi), probability of y such that x has already happened \n",
    "- For the neural network, we want the probability assigned to the actually observed next character to be high.\n",
    "\n",
    "- Now for finding this probs we used weight W. Thus ultimately the probabilities depends on the weights  \n",
    "Likelihood is defined as ->\n",
    "- Likelihood is how probable the observed labels are, given the inputs, as a function of the model parameters W\n",
    "\n",
    "\n",
    "- Since the observed data (xi,yi) is fixed, and the probabilities vary as we change  W, we view this quantity as a function of the parameters.\n",
    "\n",
    "``` \n",
    "Comparing with Probability. \n",
    "Probability asks: how likely is this data?\n",
    "Likelihood asks: which parameters make this data likely?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220a40d",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0df56",
   "metadata": {},
   "source": [
    "- We will use back propagation to update the parameters in order to minimize the loss i.e Maximize log likelihood.\n",
    "- we need high likelihood. high likelihood means our parameters are good. -> Maximize likelihood  \n",
    "- so because gradient descent minimizes the objective we take negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b5de9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5401)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0411c2",
   "metadata": {},
   "source": [
    "#### What does gradients are and what they say?\n",
    "\n",
    "- If you increase W a tiny bit in this direction, the loss will increase this much\n",
    "\n",
    "#### Why not subtarct gradient directly?\n",
    "\n",
    "- Because gradient tells us direction and not step size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8222e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# x -> characters preceding the target character\n",
    "xs = []\n",
    "\n",
    "# y label -> target character\n",
    "ys = []\n",
    "\n",
    "# Check for only first name emma\n",
    "for name in names:\n",
    "    chs = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)     \n",
    "\n",
    "num = xs.nelement()\n",
    "print(\"Number of examples: \", num)\n",
    "\n",
    "# intialize the network\n",
    "# randomly initialize 27 neurons weights. each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f68820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953094482422\n",
      "3.371100664138794\n",
      "3.154043197631836\n",
      "3.020373821258545\n",
      "2.927711248397827\n",
      "2.8604023456573486\n",
      "2.8097290992736816\n",
      "2.7701022624969482\n",
      "2.7380731105804443\n",
      "2.711496353149414\n",
      "2.6890032291412354\n",
      "2.6696884632110596\n",
      "2.6529300212860107\n",
      "2.638277292251587\n",
      "2.6253879070281982\n",
      "2.613990545272827\n",
      "2.60386323928833\n",
      "2.5948216915130615\n",
      "2.5867116451263428\n",
      "2.5794036388397217\n",
      "2.572789192199707\n",
      "2.5667762756347656\n",
      "2.5612881183624268\n",
      "2.5562589168548584\n",
      "2.551633596420288\n",
      "2.547366142272949\n",
      "2.543415069580078\n",
      "2.5397486686706543\n",
      "2.536336660385132\n",
      "2.533154249191284\n",
      "2.5301806926727295\n",
      "2.5273966789245605\n",
      "2.5247862339019775\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810489654541\n",
      "2.513878345489502\n",
      "2.512052059173584\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071346759796143\n",
      "2.5056614875793457\n",
      "2.504261016845703\n",
      "2.5029289722442627\n",
      "2.5016613006591797\n",
      "2.5004520416259766\n",
      "2.4992988109588623\n",
      "2.498197317123413\n",
      "2.497144937515259\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(50):\n",
    "    # Forward pass\n",
    "    x_enc = F.one_hot(xs, num_classes= 27).float()  # input to the neural network\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/ counts.sum(1, keepdim=True) # Probabilites of next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebd0c7",
   "metadata": {},
   "source": [
    "### Sampling from our neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c44ff59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce43258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "mogllurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([ix]), num_classes= 27).float()\n",
    "        logits = x_enc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7c403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
